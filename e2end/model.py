#!/usr/bin/env python
# -*- coding: utf-8 -*-
import numpy as np
import tensorflow as tf

import logging, math
import tensorflow.python.ops.seq2seq
from tensorflow.python.ops import control_flow_ops
from .utils import elapsed_timer
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


def lengths2mask2d(lengths, max_len):
    # Filled the row of 2d array with lengths
    lengths_transposed = tf.expand_dims(lengths, 1)
    lengths_tiled = tf.tile(lengths_transposed, [1, max_len])

    # Filled the rows of 2d array 0, 1, 2,..., max_len ranges 
    rng = tf.range(0, max_len, 1)
    range_row = tf.expand_dims(rng, 0)
    range_tiled = tf.tile(range_row, [4, 1])

    # Use the logical operations to create a mask
    return tf.to_float(tf.less(range_tiled, lengths_tiled))


def embedding_attention_decoder(decoder_inputs, initial_state, attention_states,
                                cell, num_symbols, embedding_size, num_heads=1,
                                output_size=None, output_projection=None,
                                feed_previous=False,
                                update_embedding_for_previous=True,
                                dtype=dtypes.float32, scope=None,
                                initial_state_attention=False):
    """RNN decoder with embedding and attention and a pure-decoding option.

    Args:
    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).
    initial_state: 2D Tensor [batch_size x cell.state_size].
    attention_states: 3D Tensor [batch_size x attn_length x attn_size].
    cell: rnn_cell.RNNCell defining the cell function.
    num_symbols: Integer, how many symbols come into the embedding.
    embedding_size: Integer, the length of the embedding vector for each symbol.
    num_heads: Number of attention heads that read from attention_states.
    output_size: Size of the output vectors; if None, use output_size.
    output_projection: None or a pair (W, B) of output projection weights and
      biases; W has shape [output_size x num_symbols] and B has shape
      [num_symbols]; if provided and feed_previous=True, each fed previous
      output will first be multiplied by W and added B.
    feed_previous: Boolean; if True, only the first of decoder_inputs will be
      used (the "GO" symbol), and all other decoder inputs will be generated by:
        next = embedding_lookup(embedding, argmax(previous_output)),
      In effect, this implements a greedy decoder. It can also be used
      during training to emulate http://arxiv.org/abs/1506.03099.
      If False, decoder_inputs are used as given (the standard decoder case).
    update_embedding_for_previous: Boolean; if False and feed_previous=True,
      only the embedding for the first symbol of decoder_inputs (the "GO"
      symbol) will be updated by back propagation. Embeddings for the symbols
      generated from the decoder itself remain unchanged. This parameter has
      no effect if feed_previous=False.
    dtype: The dtype to use for the RNN initial states (default: tf.float32).
    scope: VariableScope for the created subgraph; defaults to
      "embedding_attention_decoder".
    initial_state_attention: If False (default), initial attentions are zero.
      If True, initialize the attentions from the initial state and attention
      states -- useful when we wish to resume decoding from a previously
      stored decoder state and attention states.

    Returns:
    A tuple of the form (outputs, state), where:
      outputs: A list of the same length as decoder_inputs of 2D Tensors with
        shape [batch_size x output_size] containing the generated outputs.
      state: The state of each decoder cell at the final time-step.
        It is a 2D Tensor of shape [batch_size x cell.state_size].

    Raises:
    ValueError: When output_projection has the wrong shape.
    """
    if output_size is None:
        output_size = cell.output_size
    if output_projection is not None:
        proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)
        proj_biases.get_shape().assert_is_compatible_with([num_symbols])

    with variable_scope.variable_scope(scope or "embedding_attention_decoder"):
        with ops.device("/cpu:0"):
            embedding = variable_scope.get_variable("embedding", [num_symbols, embedding_size])

        # TODO replace _extract_argmax_and_embed with sample and embed ideally multiple times
        # implement switch use_inputs, feed_previous, sample
        loop_function = _extract_argmax_and_embed(embedding, output_projection, 
                update_embedding_for_previous) if feed_previous else None
    emb_inp = [
        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]
    return tensorflow.python.ops.seq2seq.attention_decoder(
        emb_inp, initial_state, attention_states, cell, output_size=output_size,
        num_heads=num_heads, loop_function=loop_function,
        initial_state_attention=initial_state_attention)


class E2E_property_decoding():

    def __init__(self, config):
        c = config  # shortcut as it is used heavily
        single_cell = tf.nn.rnn_cell.GRUCell(c.encoder_size)
        encoder_cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * c.encoder_layers) if c.encoder_layers > 1 else single_cell
        single_cell = tf.nn.rnn_cell.GRUCell(c.decoder_size)
        decoder_cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * c.decoder_layers) if c.decoder_layers > 1 else single_cell

        logger.info('Compiling %s', self.__class__.__name__)
        logger.info('FIXME NOT USING BATCHES')
        logger.info('FIXME NOT Storing data to graph. Inspired by "Preload data" section in https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html')

        logger.debug('For each word_i from i in 1..max_turn_len there is a list of features: word, belongs2slot1, belongs2slot2, ..., belongs2slotK')

        logger.debug('Feature list uses placelhoder.name to create feed dictionary')
        logger.debug('Words are the most important features')
        self.feat_list = feat_list = [tf.placeholder(tf.int64, shape=(batch_size, c.max_turn_len), name='words')]
        logger.debug('Indicators if a word belong to a slot - like abstraction - densify data')
        for i in range(c.num_slots):
            feat = tf.placeholder(tf.int64, shape=(batch_size, c.max_turn_len), name='slots_indicators{}'.format(i))
            feat_list.append(feat)
        logger.debug('Another feature for each word we have speaker id')
        feat_list.append(tf.placeholder(tf.int64, shape=(batch_size, c.max_turn_len)))

        self.turn_len = turn_len = tf.placeholder(tf.int64, shape=(batch_size,))
        input_mask = lengths2mask2d(self.turn_len)

        logger.debug('The decoder uses special token GO_ID as first input. Adding to vocabulary.')
        self.GO_ID = len(c.input_vocabs[0])
        self.goid = tf.constant(self.GO_ID)
        self.decoder_inputs = tf.placeholder(tf.int64, shape=(batch_size, c.max_target_len))
        goid_batch_vec = tf.constant([self.GO_ID] * c.batch_size, shape=(batch_size, 1))
        logger.debug('Adding GO_ID at the beggining of each decoder input')
        decoder_inputs = tf.concat(1, [goid_batch_vec, self.decoder_inputs])

        self.decoder_lengths = dec_lens = tf.placeholder(tf.int64, shape=(batch_size,))

        self.dropout_keep_prob = drop_keep_prob = tf.placeholder('float')
        self.dropout_db_keep_prob = drop_db_keep_prob = tf.placeholder('float')

        with tf.variable_scope('encoder') as inpt_scp, elapsed_timer() as inpt_timer:
            logger.debug('embedded_inputs is a list of size c.max_turn_len with tensors of shape (batch_size, all_feature_size)') 

            feat_embeddings = []
            for i in range(len(feat_list)):
                if i == 0:  # words
                    logger.debug('Increasing the size to fit in the GO_ID id. See above')
                    voclen = c.words_vocab_len + 1
                    emb_size = c.word_embed_size
                else:  # binary features
                    voclen = 2
                    emb_size = c.feat_embed_size
                feat_embeddings.append(tf.get_variable('feat_embedding{}'.format(i), 
                    initializer=tf.random_uniform([voclen, emb_size], -math.sqrt(3), math.sqrt(3))))

            embedded_inputs = []
            for j in c.max_turn_len:
                features_j_word = []
                for i in len(feat_list):
                    embedded = tf.nn.embedding_lookup(feat_embeddings[i], feat_list[i][:, j])
                    dropped_embedded = tf.nn.dropout(embedded, drop_keep_prob)
                    features_j_word.append(dropped_embedded)
                logger.debug('For each word and binary features create a tensor with shape (batch_size, sum(all_feat_embeddings_len))')
                embedded_inputs.append(tf.concat(features_j_word))

            self.is_first_turn = placeholder(tf.bool)

            logger.debug('We get input features for each turn, to represent dialog, we need to store the state between the turns')
            dialog_state_before_turn = tf.get_variable('dialog_state_before_turn', initializer=encoder_cell, zero_state(c.batch_size))
            dialog_state_before_turn = control_flow_ops.cond(is_first_turn,
                                                      lambda: encoder_cell.zero_state(c.batch_size),
                                                      lambda: tf.assign(dialog_state_before_turn))

            words_hidden_feat, dialog_state_after_turn = tf.nn.rnn(encoder_cell, embedded_inputs, 
                    initial_state=dialog_state_before_turn, sequence_length=turn_len)
            dialog_state_before_turn = tf.assign(dialog_state_after_turn)

        with tf.variable_scope('db_encoder') as dec_scp, elapsed_timer() as db_timer:
            col_embeddings = [tf.get_variable('col_values_embedding{}'.format(i), 
                initializer=tf.random_uniform([col_vocab_size, c.col_emb_size], -math.sqrt(3), math.sqrt(3))) for col_vocab_size in col_vocab_sizes] 

            self.db_row_initializer = dbi = tf.placeholder(tf.int64, shape=(c.num_rows, c.num_cols))
            self.db_rows = db_rows = tf.Variable(dbi, trainable=False, collections=[])
            db_rows_embeddings = []
            for i in range(c.num_rows):
                row_embed = [tf.nn.dropou(tf.nn.embedding_lookup(col_embeddings[j], db_rows[i, j]), drop_db_keep_prob) for j in range(c.num_cols)]
                db_rows_embeddings.append(tf.concat(1, row_embed))
            # db_rows_embedded = tf.concat(0, db_rows_embeddings)

            # logger.debug('Building word & db_vocab attention started %.2f from DB construction start')
            # logger.debug('Simple computation because lot of inputs')
            # assert c.col_emb_size == encoder.hidden_size, 'Otherwise I cannot do scalar product'
            # words_attributes_distance = []
            # for w_hid_feat in words_hidden_feat:
            #     vocab_offset = 0
            #     for vocab_emb, vocab_size in zip(col_embeddings, col_vocab_sizes):
            #         for idx in range(vocab_size):
            #             w_emb = tf.nn.embedding_lookup(vocab_emb, idx)
            #             wT_times_db_emb_value = tf.matmul(tf.transpose(w_hid_feat), w_emb)
            #             words_attributes_distance.append(wT_times_db_emb_value)
            # words_attributes_att = tf.softmax(tf.concat(0, words_attributes_distance))
            # words_vocab_entries_att = todo_reshape_into_word_TIMES_slot_vocab_TIMES_slot_value

            def select_row(row, encoded_history, reuse=False, scope='select_row'):
                with tf.variable_scope(scope, reuse=reuse):
                    inpt = tf.concat(1, [row, encoded_history])
                    W1 = tf.get_variable('W1', initializer=tf.random_normal([c.col_emb_size, c.mlp_db_l1_size]))
                    b1 = tf.get_variable('b1', initializer=tf.random_normal([c.mlp_db_l1_size]))
                    layer1 = tf.nn.relu(tf.add(tf.matmul(inpt, W1), b1))

                    last_layer = layer1
                    last_layer_size = c.mlp_db_l1_size
                    Out = tf.get_variable('Out', initializer=tf.random_normal([last_layer_size, 2]))
                    b_out = tf.get_variable('b_out', initializer=tf.random_normal([2]))
                    should_be_selected_att = tf.nn.sigmoid(tf.add(tf.matmul(last_layer, Out), b_out))
                    return should_be_selected_att

            row_selected_arr = []
            for i, r in enumerate(db_rows_embeddings):
                reuse = False if i == 0 else True
                row_selected_arr.append(select_row(r, dialog_state_after_turn))

            row_selected = tf.concat(1, row_selected_arr)


        # with tf.variable_scope('decoder') as dec_scp, elapsed_timer() as dec_timer:
        #     # Take from tf/python/ops/seq2seq.py:706
        #     # First calculate a concatenation of encoder outputs to put attention on.
        #     top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])
        #                   for e in encoder_outputs]
        #     attention_states = array_ops.concat(1, top_states)
        #
        #
        #     # replace _extract_argmax_and_embed with sample and embed ideally multiple times
        #     loop_function = _extract_argmax_and_embed(
        #         embedding, output_projection,
        #         update_embedding_for_previous) if feed_previous else None
        #     emb_inp = [
        #         embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]
        #
        #
        #     cell = rnn_cell.OutputProjectionWrapper(cell, c.total_input_vocab_size)
        #     output_size = c.total_input_vocab_size
        #
        #
        #     # If feed_previous is a Tensor, we construct 2 graphs and use cond.
        #     def decoder(feed_previous_bool):
        #         reuse = None if feed_previous_bool else True
        #         with variable_scope.variable_scope(variable_scope.get_variable_scope(),
        #                                              reuse=reuse):
        #
        #             outputs, state = embedding_attention_decoder(
        #                 decoder_inputs, encoder_state, attention_states, cell,
        #                 num_decoder_symbols, embedding_size, num_heads=num_heads,
        #                 output_size=output_size, output_projection=output_projection,
        #                 feed_previous=feed_previous_bool,
        #                 update_embedding_for_previous=False,
        #                 initial_state_attention=initial_state_attention)
        #             return outputs + [state]
        #
        #     *dec_outputs, dec_state = control_flow_ops.cond(feed_previous,
        #                                       lambda: decoder(True),
        #                                       lambda: decoder(False))

        with tf.variable_scope('output') as out_scp, elapsed_timer() as out_timer:
            pass


    def _connect_optimizer(self):
        self._optimizer = opt = tf.train.GradientDescentOptimizer(self.config.learning_rate)
        self.global_step = tf.Variable(0, name='global_step', trainable=False)
        tf.scalar_summary(self.loss.op.name + 'loss', self.loss)
        self.train_op = opt.minimize(self.loss, global_step=self.global_step)

    @staticmethod
    def _build_graph(dialog, turn_lens, labels, dropout_keep_prob, c):
        slu_states = [666] * c.max_dial_len
        for t in range(c.max_dial_len):
            # FIXME separate into function
            reuse_it = True if t > 0 else None
            with tf.variable_scope('turn_encoder', reuse=reuse_it):

                forward_slu_gru = tf.nn.rnn_cell.GRUCell(c.rnn_size, input_size=c.embedding_size)
                logger.debug('c.embedding_size: %s', c.embedding_size)
                logger.debug('dropped_embedded_inputs[0].get_shape(): %s', dropped_embedded_inputs[0].get_shape())
                with tf.variable_scope('forward_slu'):
                    outputs, last_slu_state = tf.nn.rnn(
                        cell=forward_slu_gru,
                        inputs=dropped_embedded_inputs,
                        dtype=tf.float32,
                        sequence_length=turn_lens[:, t])
                    slu_states[t] = last_slu_state

        masked_turns = tf.to_int64(tf.greater(turn_lens, tf.zeros_like(turn_lens)))
        logger.debug('masked_turns.get_shape(): %s', masked_turns.get_shape())
        dial_len = tf.reduce_sum(masked_turns, 1)
        masked_turnsf = tf.to_float(masked_turns)

        forward_dst_gru = tf.nn.rnn_cell.GRUCell(c.rnn_size, input_size=c.rnn_size)  # FIXME use different rnn_size
        with tf.variable_scope('dialog_state'):
                dialog_states, last_dial_state = tf.nn.rnn(
                    cell=forward_dst_gru,
                    inputs=slu_states,
                    dtype=tf.float32,
                    sequence_length=dial_len)
        logitss = [444] * c.max_dial_len
        for t in range(c.max_dial_len):
            with tf.variable_scope('slot_prediction', reuse=True if t > 0 else None):
                # FIXME better initialization
                w_project = tf.get_variable('project2labels', 
                        initializer=tf.random_uniform([c.rnn_size, c.labels_size], -1.0, 1.0))  # FIXME dynamically change size based on the input not used fixed c.rnn_size
                logitss[t] = tf.matmul(dialog_states[t], w_project)

        logger.debug('dialog_states[0].get_shape(): %s', dialog_states[0].get_shape())
        logger.debug('w_project.get_shape(): %s', w_project.get_shape())

        logits = tf.reshape(tf.concat(1, logitss), (np.prod(masked_turns.get_shape().as_list()), c.labels_size)) 
        logger.debug('logits.get_shape(): %s', logits.get_shape())

        with tf.variable_scope('loss'):
            logger.debug('labels.get_shape(): %s', labels.get_shape())
            masked_logits = tf.mul(logits, tf.reshape(masked_turnsf, (np.prod(masked_turnsf.get_shape().as_list()), 1)))
            logger.debug('masked_logits.get_shape(): %s, masked_logits.dtype %s', masked_logits.get_shape(), masked_logits.dtype)
            labels_vec = tf.reshape(labels, [-1])
            xents = tf.nn.sparse_softmax_cross_entropy_with_logits(masked_logits, labels_vec)
            logger.debug('xents.get_shape(): %s, dtype %s', xents.get_shape(), xents.dtype)
            loss = tf.reduce_sum(xents) / tf.reduce_sum(masked_turnsf)

        with tf.variable_scope('eval'):
            predicts = tf.argmax(masked_logits, 1)
            true_count = tf.reduce_sum(tf.to_int64(tf.equal(predicts, labels_vec)) * tf.reshape(masked_turns, [-1]))
            num_turns = tf.reduce_sum(dial_len)
            batch_accuracy = tf.div(tf.to_floa(true_count), tf.to_float(num_turns))
            logger.debug('true_count.get_shape(): %s', true_count.get_shape())
        logger.info('trainable variables: %s', '\n'.join([str((v.name, v.get_shape())) for v in tf.trainable_variables()]))
        return (predicts, loss, num_turns, true_count, batch_accuracy)

    def train_step(self, session, todo_inputs, sample=False):
        pass

    def eval_step(self, session, todo_inpouts, gold_labels=None):
        pass
